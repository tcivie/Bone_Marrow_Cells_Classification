{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcivie/Bone_Marrow_Cells_Classification/blob/main/AI_Project_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqet-OPajTl1"
      },
      "source": [
        "#Start by converting the jpeg files to TFRecords\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing necessary modules and libraries\n",
        "We first import the necessary modules and libraries, such as os, tensorflow, PIL, numpy, and keras. These modules and libraries are used throughout the notebook for various tasks such as reading and manipulating images, file management, and data preprocessing."
      ],
      "metadata": {
        "id": "HGbr4j8XnJb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46YQ5nNCh_ZY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import re\n",
        "import numpy as np\n",
        "import keras\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww9X9IM0jp5J"
      },
      "source": [
        "##Mounting the Google Drive folder\n",
        "We then use the google.colab.drive module to mount the Google Drive folder where the images are stored. The folder is mounted at the location `/content/drive/MyDrive/BM_cytomorphology_data/`. This allows the notebook to access the dataset and perform operations on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9V_-5YajhhB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFgXztHLj_gv"
      },
      "outputs": [],
      "source": [
        "images_folder = '/content/drive/MyDrive/BM_cytomorphology_data/'\n",
        "tfrecords_folder = '/content/drive/MyDrive/BM_cytomorphology_data_tf'\n",
        "images_in_tf = 230"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpO3xpKalXf5"
      },
      "source": [
        "##Defining the classes/folders\n",
        "In this code snippet we define a list of `CLASSES`, which contains the names of all the classes/folders in the dataset. We also define a list of `SMALL_CLASSES`, which contains the names of the classes that have a deficiency of data. These lists are used to access the specific folders and perform operations on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvMyNEtykqEd"
      },
      "outputs": [],
      "source": [
        "CLASSES = [\"ABE\", \"ART\", \"BAS\", \"BLA\", \"EBO\", \"EOS\", \"FGC\", \"HAC\", \"KSC\", \"LYI\", \"LYT\", \"MMZ\", \"MON\", \"MYB\", \"NGB\",\n",
        "              \"NGS\", \"NIF\", \"OTH\", \"PEB\", \"PLM\", \"PMO\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SMALL_CLASSES = ['ABE','BAS','FGC','HAC','KSC','LYI','OTH']"
      ],
      "metadata": {
        "id": "mIu6v949ZtT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating additional data\n",
        "Here we use the `keras.preprocessing.image.ImageDataGenerator` module to create more data for the `SMALL_CLASSES`. The module applies different data augmentation techniques such as rotation, rescaling, zoom, and flipping on the original images. These newly generated images are saved to the same folder as the original images with a prefix \"augmented_\". This step is done to balance the data, by creating more data for the underrepresented classes."
      ],
      "metadata": {
        "id": "-puitL6xYMSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = keras.preprocessing.image.ImageDataGenerator(rotation_range =15,\n",
        "                         rescale=1./255,\n",
        "                         zoom_range=0.2, \n",
        "                         horizontal_flip = True,\n",
        "                         vertical_flip = True,\n",
        "                         fill_mode = 'nearest') \n",
        "\n",
        "for class_name in SMALL_CLASSES:\n",
        "  class_path = os.path.join(images_folder,class_name)\n",
        "  images_list = os.listdir(class_path)\n",
        "\n",
        "  # max_index = images_list.map(lambda name: int(re.match('[0-9]{5}',name))) # convert the list of filenames to list of indexes\n",
        "  # max_index = max(max_index) + 1\n",
        "  number_of_images = len(images_list)\n",
        "  print(number_of_images)\n",
        "  total_images = number_of_images\n",
        "  for image in images_list:\n",
        "    if total_images >= 4040:\n",
        "      break\n",
        "    i = 0\n",
        "    image = os.path.join(class_path,image)\n",
        "    print(image)\n",
        "    image = keras.preprocessing.image.image_utils.img_to_array(Image.open(image))\n",
        "    image = image.reshape((1,) + image.shape)\n",
        "\n",
        "    for batch in datagen.flow(image, batch_size=1, save_prefix='augmented_', save_to_dir =class_path, save_format='jpg'):\n",
        "      total_images += 1\n",
        "      i += 1\n",
        "      if i == number_of_images or total_images >= 4040:\n",
        "        break"
      ],
      "metadata": {
        "id": "USpnhxg6Ydap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Counting the number of files in each folder\n",
        "We then count the number of files in each folder to check that the data is balanced. This is done by iterating through the list of `CLASSES` and counting the number of files in each folder using the `os.listdir()` function."
      ],
      "metadata": {
        "id": "M_PPrBl4jkoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in CLASSES:\n",
        "  class_path = os.path.join(images_folder,class_name)\n",
        "  images_list = os.listdir(class_path)\n",
        "  print(class_name + \":\" + str(len(images_list)))"
      ],
      "metadata": {
        "id": "fCm_FGaK4FJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Renaming the newly generated files\n",
        "Finally, the newly generated files are renamed to match the regular ones. This is done by using regular expressions to find the existing file names and extract their indexes, then the indexes are used to rename the newly generated files. This step is done to ensure that the newly generated files are in the same format as the original files and can be easily used for training the CNN."
      ],
      "metadata": {
        "id": "yWpx2VjojtxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in SMALL_CLASSES:\n",
        "  class_path = os.path.join(images_folder,class_name)\n",
        "  images_list = os.listdir(class_path)\n",
        "\n",
        "  reg = re.compile(r'[A-Z]{3}_[0-9]{5}')\n",
        "  previous_fies = list(filter(reg.search, images_list))\n",
        "\n",
        "  max_index = list()\n",
        "  for file_name in previous_fies:\n",
        "    max_index.append(int(re.findall(r'[0-9]{5}',file_name)[0])) # convert the list of filenames to list of indexes\n",
        "  # max_index = previous_fies.map(lambda name: int(re.match('[0-9]{5}',name))) # convert the list of filenames to list of indexes\n",
        "  max_index = max(max_index) + 1\n",
        "   \n",
        "  list_of_files_to_rename = list(filter(lambda x: not reg.search(x), images_list))\n",
        "\n",
        "  print(class_name,max_index,list_of_files_to_rename)\n",
        "\n",
        "  for file_name in list_of_files_to_rename:\n",
        "    os.rename(os.path.join(class_path,file_name),os.path.join(class_path,class_name + '_' + str(max_index).rjust(5,'0') + '.jpg'))\n",
        "    max_index += 1\n",
        "  print(class_name,max_index,list_of_files_to_rename)"
      ],
      "metadata": {
        "id": "rJD3clpKjzVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoding data to TFRecords\n",
        "The following part is used to encode the images in the dataset into the TFRecords format. The TFRecords format is a binary file format used to store data for TensorFlow."
      ],
      "metadata": {
        "id": "Se2RZqEKh38B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining helper functions\n",
        "The first part of the code defines three helper functions:\n",
        "\n",
        "* `_bytestring_feature`: This function takes a list of bytestrings as an input \n",
        "and returns a tf.train.Feature object containing the bytestrings.\n",
        "* `_int_feature`: This function takes a list of integers as an input and returns a tf.train.Feature object containing the integers.\n",
        "* `_float_feature`: This function takes a list of floats as an input and returns a tf.train.Feature object containing the floats."
      ],
      "metadata": {
        "id": "HgLyucTEoIOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjDWmzg1UTOk"
      },
      "outputs": [],
      "source": [
        "def _bytestring_feature(list_of_bytestrings):\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "def _int_feature(list_of_ints): # int64\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "def _float_feature(list_of_floats): # float32\n",
        "  return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create example function\n",
        "The `create_example` function takes an image and its label as input, and returns a `tf.train.Example` object. The function first converts the class label into a one-hot encoded array, which is a binary array used to represent the class label. The function then creates a feature dictionary containing the following keys:\n",
        "\n",
        "* \"image\": a bytestring feature containing the image data.\n",
        "* \"class\": an int feature containing the class number.\n",
        "* \"one_hot_class\": a float feature containing the one-hot encoded class label."
      ],
      "metadata": {
        "id": "riJ0MSeLoaYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example(image, label):\n",
        "    class_num = np.argmax(np.array(CLASSES)==label) # 'ART' => 1 (order defined in CLASSES)\n",
        "    one_hot_class = np.eye(len(CLASSES))[class_num]     # [0, 1, 0, 0, 0] for class #2, cells\n",
        "    feature = {\n",
        "      \"image\": _bytestring_feature([image]), # one image in the list\n",
        "      \"class\": _int_feature([class_num]),        # one class in the list\n",
        "      \"one_hot_class\": _float_feature(one_hot_class.tolist()) # variable length  list of floats, n=len(CLASSES)\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))"
      ],
      "metadata": {
        "id": "JVM0GZ9LoZ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Writing the data to TFRecords\n",
        "The rest of the code snippet iterates through the list of `CLASSES`, and for each class:\n",
        "\n",
        "* It creates a list of image file names in the class folder, by using the re module to match the file names with a specific pattern.\n",
        "* It groups the image file names into small lists, each containing the number of images defined by the variable 'images_in_tf'.\n",
        "* It creates a new TFRecords file and writes the images in each small list to the file, by using the tf.io.TFRecordWriter module.\n",
        "\n",
        "This block of code is reading all the images from the directory and creating tfrecords files from them, it also groups them in specific number of files to make sure the the size of the files are manageable."
      ],
      "metadata": {
        "id": "no0USsNwoi4H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efd1wM2HU1Zk"
      },
      "outputs": [],
      "source": [
        "counted_images = 0 # To make sure we are not exceeding the number of images in tf\n",
        "record_file_number = 0\n",
        "record_file_name = (\"train.tfrecords-%.3d\" % record_file_number)\n",
        "\n",
        "for class_name in CLASSES:\n",
        "  print(class_name)\n",
        "  class_path = os.path.join(images_folder,class_name)\n",
        "\n",
        "  images_list = os.listdir(class_path)\n",
        "  regex = re.compile(r'[0-9]{5}\\.jpg$')\n",
        "  images_list = [i for i in images_list if not regex.match(i)]\n",
        "  print(images_list)\n",
        "\n",
        "  samples = list()\n",
        "  for i in range(0, len(images_list), images_in_tf):\n",
        "    samples.append(images_list[i:i+images_in_tf])\n",
        "  for sample_list in samples:\n",
        "    with tf.io.TFRecordWriter(\n",
        "          tfrecords_folder + \"/file_%.3i.tfrec\" % record_file_number\n",
        "      ) as writer:\n",
        "        print(tfrecords_folder + \"/file_%.3i.tfrec\" % record_file_number)\n",
        "        record_file_number += 1\n",
        "        for sample in sample_list:\n",
        "            image_path = os.path.join(class_path,sample)\n",
        "            image = open(image_path,'rb').read()\n",
        "            example = create_example(image, class_name)\n",
        "            writer.write(example.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lgqpwqgpGt_"
      },
      "source": [
        "## File uploading to the google data storage\n",
        "Here we upload the TFRecords files to a Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Authenticating the user\n",
        "Here we use the `google.colab.auth` module to authenticate. This is necessary to give the notebook access to the Google Cloud Storage bucket."
      ],
      "metadata": {
        "id": "LC3usL8qo6cC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1rAzcpjqdq-"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uploading files to the bucket\n",
        "Here we use a shell command that uses the `gsutil` command-line tool to upload the TFRecords files to the bucket. The `-m` flag is used to perform a parallel upload, which speeds up the upload process. The `-r` flag is used to perform a recursive upload, which uploads all the files in the specified directory and its subdirectories. The `cp` command is used to copy the files to the bucket. The source directory is `/content/drive/MyDrive/BM_cytomorphology_data_tf/` and the destination is the bucket `gs://bm_cytomorphology_data_tf/`.\n",
        "\n",
        "This block of code is uploading the created tfrecords files from the local storage to the google storage bucket. This will allow the data to be easily accessible for any other machine learning tasks that may need it."
      ],
      "metadata": {
        "id": "5nBVHAOdpAHW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMPDPkIBpzrQ"
      },
      "outputs": [],
      "source": [
        "!gsutil -m cp -r /content/drive/MyDrive/BM_cytomorphology_data_tf/ gs://bm_cytomorphology_data_tf/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10G5SlEmrOwn"
      },
      "source": [
        "#Create CNN using Google TPU\n",
        "This code snippet is used to create and train a convolutional neural network (CNN) using Google TPU and TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89B27-TGiDNB"
      },
      "source": [
        "##Imports\n",
        "First, the necessary libraries and modules are imported, including TensorFlow, Numpy, and Matplotlib. The version of TensorFlow is also printed to ensure that it is the correct version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z55vYn_erq46"
      },
      "outputs": [],
      "source": [
        "import re, time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPo10cahZXXQ"
      },
      "source": [
        "## TPU or GPU detection\n",
        "The code then detects whether a TPU or GPU is available for training the model. If a TPU is available, it connects to it and creates a `tf.distribute.TPUStrategy` object. If a TPU is not available, it creates a `tf.distribute.MirroredStrategy` object for training on a GPU or multiple GPUs. The number of available accelerators (TPUs or GPUs) is also printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "outputs": [],
      "source": [
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "    # strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    # strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n",
        "\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9S3uKC_iXY5"
      },
      "source": [
        "## Configuration\n",
        "We then set some configuration variables, such as the number of training epochs, the image size, and the list of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6BkH8FtYsR"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 12\n",
        "IMAGE_SIZE = [250, 250]\n",
        "\n",
        "DATASET = 'gs://bm_cytomorphology_data_tf/BM_cytomorphology_data_tf/*'\n",
        "CLASSES = [\"ABE\", \"ART\", \"BAS\", \"BLA\", \"EBO\", \"EOS\", \"FGC\", \"HAC\", \"KSC\", \"LYI\", \"LYT\", \"MMZ\", \"MON\", \"MYB\", \"NGB\",\n",
        "              \"NGS\", \"NIF\", \"OTH\", \"PEB\", \"PLM\", \"PMO\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a41SQKZt7ju"
      },
      "source": [
        "###Mixed precision\n",
        "Mixed precision training is a technique that uses lower-precision data types (such as bfloat16) for some model variables and computations to save memory and improve computation speed. If the `MIXED_PRECISION` variable is set to True, the code enables mixed precision training by setting a global policy for TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWVvt13CuF1h"
      },
      "outputs": [],
      "source": [
        "MIXED_PRECISION = False\n",
        "if MIXED_PRECISION:\n",
        "    if tpu: \n",
        "        policy = tf.keras.mixed_precision.Policy('mixed_bfloat16')\n",
        "    else:\n",
        "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "        tf.config.optimizer.set_jit(True) # XLA compilation\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print('Mixed precision enabled')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x29_G5y8uOGc"
      },
      "source": [
        "###Batch and learning rate settings\n",
        "The code then sets batch size and learning rate settings based on the number of available accelerators. These settings are used during the training process to control the number of examples processed at a time and the rate at which the model learns from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUh75wJUuQSz"
      },
      "outputs": [],
      "source": [
        "if strategy.num_replicas_in_sync == 8: # TPU or 8xGPU\n",
        "    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00005 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "elif strategy.num_replicas_in_sync == 1: # single GPU\n",
        "    BATCH_SIZE = 16\n",
        "    VALIDATION_BATCH_SIZE = 16\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.0002\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "else: # TPU pod\n",
        "    BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00002 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 7\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_Ro_F2uxAT"
      },
      "source": [
        "###Define learning function and plot the learning curve\n",
        "The code defines a function 'lrfn' that takes an epoch as input and returns the learning rate for that epoch. The learning rate is calculated based on the current epoch, the starting learning rate, the minimum learning rate, the maximum learning rate, the number of ramp-up epochs, the number of sustain epochs, and an exponential decay rate. The function first checks if the current epoch is less than the number of ramp-up epochs, in which case the learning rate increases linearly from the starting learning rate to the maximum learning rate over the ramp-up period. If the current epoch is greater than the ramp-up period but less than the number of sustain epochs, the learning rate remains at the maximum value. If the current epoch is greater than the sum of the ramp-up and sustain epochs, the learning rate decays exponentially towards the minimum learning rate.\n",
        "\n",
        "The code also creates a callback function 'lr_callback' which wraps the 'lrfn' function and is used to schedule the learning rate during training. Finally, the code plots the learning curve by generating an array of the learning rates for each epoch and plotting it using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zODOKm6xu3cz"
      },
      "outputs": [],
      "source": [
        "def lrfn(epoch): # Learning function\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, [lrfn(x) for x in rng])\n",
        "print(y[0], y[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Display utilities\n",
        "The code defines several utility functions for displaying images and image data.\n",
        "\n",
        "The function `dataset_to_numpy_util` takes a dataset and a number of images as input and returns the first N images and labels from the dataset as numpy arrays.\n",
        "\n",
        "The function `title_from_label_and_target` takes a label and a correct label as input, converts them from one-hot encoding to class numbers, and returns a string that contains the label, whether the label is correct and the correct label.\n",
        "\n",
        "The function `display_one_cell` takes an image, a title, a subplot index, and a red flag as input, and displays the image in the specified subplot with the specified title.\n",
        "\n",
        "The function `display_9_images_from_dataset` takes a dataset as input and displays the first 9 images and labels from the dataset in a 3x3 grid.\n",
        "\n",
        "The function `display_9_images_with_predictions` takes a list of images, a list of predictions, and a list of labels as input and displays the images in a 3x3 grid with their corresponding predictions and labels."
      ],
      "metadata": {
        "id": "XLxMHuUDs3k_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPkvHdAYNt9J"
      },
      "outputs": [],
      "source": [
        "def dataset_to_numpy_util(dataset, N):\n",
        "  print(\"dataset_to_numpy_util: \")\n",
        "  print(dataset)\n",
        "  dataset = dataset.unbatch().batch(N)\n",
        "  for images, labels in dataset:\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    break;  \n",
        "  return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  # print(\"title_from_label_and_target: \" + label + \"\\n\" + correct_label)\n",
        "  label = np.argmax(label, axis=-1)  # one-hot to class number\n",
        "  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_cell(image, title, subplot, red=False):\n",
        "  print(\"display_one_cell:\")\n",
        "  print(type(image))\n",
        "  print(title)\n",
        "  plt.subplot(subplot)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "  return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset):\n",
        "  # print(\"display_9_images_from_dataset: \")\n",
        "  # print(dataset)\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
        "    subplot = display_one_cell(image, title, subplot)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  # print(\"display_9_images_with_predictions: \")\n",
        "  # print(images)\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "    subplot = display_one_cell(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    #plt.tight_layout() # bug in tight layout in this version of matplotlib\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  #ax.set_ylim(0.28,1.05)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvPXiovhi3ZZ"
      },
      "source": [
        "#Data Loading and Training\n",
        "We start by importing the Google colab package and authenticating the user. This is necessary for accessing the data from the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSbO_a3e5zlE"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the data into train and validation sets\n",
        "We then define a function `count_data_items(filenames)` to count the number of data items in the dataset. The number of data items is written in the name of the .tfrec files.\n",
        "\n",
        "A validation split of 20% is defined and the filenames are shuffled randomly. Then, the dataset is split into training and validation sets using the validation split defined earlier. We also define the number of steps per epoch for the training dataset and print the number of training and validation images."
      ],
      "metadata": {
        "id": "iMNByvu5u7bB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T96bapUevm8n"
      },
      "outputs": [],
      "source": [
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files\n",
        "    n = [int(re.compile(r\"_([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "validation_split = 0.20\n",
        "filenames = tf.io.gfile.glob(DATASET)\n",
        "\n",
        "import random\n",
        "random.shuffle(filenames)\n",
        "\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "TRAIN_FILENAMES = filenames[:split]\n",
        "VALID_FILENAMES = filenames[split:]\n",
        "TRAIN_STEPS = count_data_items(TRAIN_FILENAMES) // BATCH_SIZE\n",
        "print(\"TRAINING IMAGES: \", count_data_items(TRAIN_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\n",
        "print(\"VALIDATION IMAGES: \", count_data_items(VALID_FILENAMES))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading data from TFRecords\n",
        "We define a function `read_tfrecord(example)` to read the data from the TFRecords. The function receives an example and parse it using the feature definition. The features are \"image\", \"class\" and \"one_hot_class\". The function then decodes the image, casts it to float and scales it to the range [0, 1]. Finally, the function returns the image and one_hot_class."
      ],
      "metadata": {
        "id": "Xwq5u0tavEQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX79uB7Oz6m3"
      },
      "outputs": [],
      "source": [
        "def read_tfrecord(example):\n",
        "  print(\"read_tfrecord:\" + example)\n",
        "  features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  print(example)\n",
        "  image = tf.io.decode_jpeg(example['image'], channels=3)\n",
        "  image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "  class_label = tf.cast(example['class'], tf.int32)\n",
        "  one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n",
        "  one_hot_class = tf.reshape(one_hot_class, [len(CLASSES)])\n",
        "  return image, one_hot_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxz1nRgh28kM"
      },
      "source": [
        "##Resizing images\n",
        "Explicit size will be needed for TPU.\n",
        "\n",
        "We define a function `force_image_sizes(dataset, image_size)` that reshapes the images to the image size defined in the argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woL9mii12yxK"
      },
      "outputs": [],
      "source": [
        "def force_image_sizes(dataset, image_size):\n",
        "    print(\"force_image_sizes:\")\n",
        "    print(dataset)\n",
        "    reshape_images = lambda image, label: (tf.reshape(image, [*image_size, 3]), label)\n",
        "    dataset = dataset.map(reshape_images, num_parallel_calls=AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading dataset\n",
        "We define a function `load_dataset(filenames)` that loads the dataset from the filenames passed as an argument. The function reads the dataset using TFRecordDataset and applies the read_tfrecord function to it. Then the dataset is reshaped using the force_image_sizes function."
      ],
      "metadata": {
        "id": "_RW48v7SvS4l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4BB0CoM2-mm"
      },
      "outputs": [],
      "source": [
        "def load_dataset(filenames):\n",
        "    print(\"load_dataset:\")\n",
        "    print(filenames)\n",
        "    # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\n",
        "    # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\n",
        "    # to allow order-altering optimizations.\n",
        "\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_deterministic = False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "    dataset = dataset.with_options(opt)\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = force_image_sizes(dataset, IMAGE_SIZE)\n",
        "    print(dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data augmentation\n",
        "We define a function `data_augment(image, one_hot_class)` that performs data augmentation on the input image. The function applies random flip and random crop to the image and returns the augmented image and one_hot_class."
      ],
      "metadata": {
        "id": "TrA5W25RvYbC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usdOEG5k3KC9"
      },
      "outputs": [],
      "source": [
        "def data_augment(image, one_hot_class):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_crop(image, size=[250,250,3])\n",
        "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
        "    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "    return image, one_hot_class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to create a training dataset\n",
        "###Loading the training dataset\n",
        "\n",
        "We first load the training dataset by calling the `load_dataset()` function and passing in the `TRAIN_FILENAMES` variable.\n",
        "###Data Augmentation\n",
        "\n",
        "We apply data augmentation to the dataset by calling the `data_augment()` function and passing the dataset to it. This will randomly flip and crop the images in the dataset.\n",
        "###Repeat and Shuffle\n",
        "\n",
        "We repeat the dataset indefinitely and shuffle it to ensure randomness in the images during training.\n",
        "###Batching\n",
        "\n",
        "We batch the dataset with a batch size of `BATCH_SIZE` so that the model can process multiple images at once.\n",
        "###Prefetching\n",
        "\n",
        "We prefetch the next batch of images while training using `dataset.prefetch(AUTOTUNE)`. This will improve the performance of the model by reducing the time taken to load the next batch of images."
      ],
      "metadata": {
        "id": "YE-Yd3l4v2Mz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox9EbMBu3sso"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    print(\"get_training_dataset:\")\n",
        "    print(dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function to create a validation dataset\n",
        "###Loading the validation dataset\n",
        "\n",
        "We first load the validation dataset by calling the `load_dataset()` function and passing in the `VALID_FILENAMES` variable.\n",
        "###Batching\n",
        "\n",
        "We batch the dataset with a batch size of `VALIDATION_BATCH_SIZE` so that the model can process multiple images at once.\n",
        "###Prefetching\n",
        "\n",
        "We prefetch the next batch of images while training using d`ataset.prefetch(AUTOTUNE)`. This will improve the performance of the model by reducing the time taken to load the next batch of images.\n",
        "###TPU Sharding\n",
        "\n",
        "We set the sharding policy to `tf.data.experimental.AutoShardPolicy.DATA` to disable file sharding policy for TPU 32-core pods."
      ],
      "metadata": {
        "id": "DrybivWawIwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwcm0XNB3vZA"
      },
      "outputs": [],
      "source": [
        "def get_validation_dataset():\n",
        "    dataset = load_dataset(VALID_FILENAMES)\n",
        "    dataset = dataset.batch(VALIDATION_BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    \n",
        "    # needed for TPU 32-core pod: the test dataset has only 3 files but there are 4 TPUs. FILE sharding policy must be disabled.\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "    dataset = dataset.with_options(opt)\n",
        "    print(\"get_validation_dataset:\")\n",
        "    print(dataset)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22rVDTx8wCqE"
      },
      "source": [
        "##Creating training and validation datasets\n",
        "We use the `get_training_dataset()` and `get_validation_dataset()` functions to create the training and validation datasets, respectively. These datasets are composed by loading the data from the TFRecords files, applying data augmentation and shuffling on the training dataset and batching the data. We also prefetch the next batch of data to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wxKyCklR4Gh"
      },
      "outputs": [],
      "source": [
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Displaying a sample of the validation dataset\n",
        "We use the `display_9_images_from_dataset()` function to display a sample of the validation dataset, allowing us to visually inspect the data."
      ],
      "metadata": {
        "id": "jqZfX6RxwioH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb-b4PRz-V6O"
      },
      "outputs": [],
      "source": [
        "display_9_images_from_dataset(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "## Model\n",
        "This cell creates a model using the DenseNet201 (We also have other architecures we can use. The DenseNet201 architecture with pre-trained weights from ImageNet. The model is composed of the pre-trained model with additional layers of global average pooling and a dense layer with softmax activation for output. The model is then compiled using the Adam optimizer, categorical crossentropy loss, and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJl3vNtJOB-x"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    #pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "    #pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "    #pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    # pretrained_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model.trainable = True\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        pretrained_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        #tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(len(CLASSES), activation='softmax', dtype=tf.float32) # the float32 is needed on softmax layer when using mixed precision\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJNVGwHUDy1"
      },
      "outputs": [],
      "source": [
        "with strategy.scope(): # creating the model in the TPUStrategy scope places the model on the TPU\n",
        "    model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "## Training\n",
        "This cell trains the model using the defined TPUStrategy and the training and validation datasets. It tracks the training time, final accuracy, and displays training curves for accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-ID7vP5mIKs"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "                    steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])\n",
        "\n",
        "final_accuracy = history.history[\"val_accuracy\"][-5:]\n",
        "print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))\n",
        "print(\"TRAINING TIME: \", time.time() - start_time, \" sec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VngeUBIdyJ1T"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['accuracy'][1:], history.history['val_accuracy'][1:], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'][1:], history.history['val_loss'][1:], 'loss', 212)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "## Predictions\n",
        "This cell creates predictions for a random set of images from the validation dataset and compares them to the true labels. It also evaluates the model on this dataset and displays 9 images with their predictions and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZcpoO4jOB-8"
      },
      "outputs": [],
      "source": [
        "# a couple of images to test predictions too\n",
        "some_cells, some_labels = dataset_to_numpy_util(validation_dataset, 160)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X07EKsqK_RW"
      },
      "outputs": [],
      "source": [
        "# randomize the input so that you can execute multiple times to change results\n",
        "permutation = np.random.permutation(8*20)\n",
        "some_cells, some_labels = (some_cells[permutation], some_labels[permutation])\n",
        "\n",
        "predictions = model.predict(some_cells, batch_size=16)\n",
        "evaluations = model.evaluate(some_cells, some_labels, batch_size=16)\n",
        "  \n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "\n",
        "display_9_images_with_predictions(some_cells, predictions, some_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIC5KKcJOB_A"
      },
      "source": [
        "## Save the model\n",
        "This cell saves the model to a Google Cloud Storage bucket after authenticating the user."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ZadD_cuejgdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivTV2qPoOB_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "SAVE_PATH = 'gs://bm_cytomorphology_data_tf/models/'\n",
        "MODEL_NAME = 'DenseNet201.h5'\n",
        "model.save(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/DenseNet201.h5 gs://bm_cytomorphology_data_tf/models/"
      ],
      "metadata": {
        "id": "YiTWomHs0jLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVgGwS-OB_K"
      },
      "source": [
        "## Reload the model\n",
        "This cell reloads the model and runs a prediction on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4AAIjcoOB_K"
      },
      "outputs": [],
      "source": [
        "reload_model = tf.keras.models.load_model(os.path.join(SAVE_PATH,MODEL_NAME))\n",
        "\n",
        "predictions = reload_model.predict(some_cells, batch_size=16)\n",
        "evaluations = reload_model.evaluate(some_cells, some_labels, batch_size=16)\n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "display_9_images_with_predictions(some_cells, predictions, some_labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Hqet-OPajTl1"
      ],
      "mount_file_id": "13OPTpX1CQ_1jvgRoOKa-iVAE_UlKwIeN",
      "authorship_tag": "ABX9TyOfJyqwWkhB0HdGXpnnCWng",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
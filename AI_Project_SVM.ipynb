{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1rpeG-CLLsboep4LihOFDXacaOazCZNtc",
      "authorship_tag": "ABX9TyOx+K/6WUif5HiypO5qEqgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcivie/Bone_Marrow_Cells_Classification/blob/main/AI_Project_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Begin monitoring memmory and make the nessecary imports for the project\n",
        "This cell installs the sysstat package, which is a command line utility that allows you to monitor system performance, and starts logging memory usage using the sar command."
      ],
      "metadata": {
        "id": "AdiRdpZzFjmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install sysstat"
      ],
      "metadata": {
        "id": "DAKYHTtt4Xwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup sar -r 1 -o sar-mem-rbf.log &"
      ],
      "metadata": {
        "id": "ERgi_nAdFmx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn as sk\n",
        "from sklearn import datasets\n",
        "import tensorflow as tf\n",
        "from sklearn import base as Bunch\n",
        "import os\n",
        "import random\n",
        "from sklearn.datasets import load_files\n",
        "import shutil\n",
        "import re"
      ],
      "metadata": {
        "id": "DLbzLYkRsg_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount Google drive\n",
        "This cell mounts the user's Google Drive to the Colab notebook file system."
      ],
      "metadata": {
        "id": "vSRD5deJWOO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "77MSvKMAWKY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configurations\n",
        "This cell sets the path for the dataset and the classes that the model will be trained on. It also sets a variable for the size of the dataset to be used and the path for the representive data."
      ],
      "metadata": {
        "id": "pW13U7YYWRW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET = 'gs://bm_cytomorphology_data_tf/BM_cytomorphology_data/*'\n",
        "DATASET = '/content/drive/MyDrive/BM_cytomorphology_data'\n",
        "CLASSES = [\"ABE\", \"ART\", \"BAS\", \"BLA\", \"EBO\", \"EOS\", \"FGC\", \"HAC\", \"KSC\", \"LYI\", \"LYT\", \"MMZ\", \"MON\", \"MYB\", \"NGB\",\n",
        "              \"NGS\", \"NIF\", \"OTH\", \"PEB\", \"PLM\", \"PMO\"]\n",
        "DATASET_SIZE = 72 # Size of each class to test (72 is the lowest number of a class)\n",
        "REPRESENTIVE_DATA_PATH = '/content/BM_cytomorphology_data_rep'"
      ],
      "metadata": {
        "id": "HDAwGmHoterJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data loading and configuration\n",
        "This cell creates a directory for the representive data and for each class, it creates a directory in this folder. Then it selects a random sample of images from the original dataset folder for each class and copies them to the representive data folder. It uses the os library to create, join and list the directories and files, random library to select the random sample of images, and shutil library to copy the images. It also checks if the folders already exist and in that case, it prints a message that the folder already exists."
      ],
      "metadata": {
        "id": "MbRoFtfNoRZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  os.mkdir(REPRESENTIVE_DATA_PATH)\n",
        "except FileExistsError:\n",
        "  print('folder already exists:' + REPRESENTIVE_DATA_PATH)\n",
        "for class_name in CLASSES:\n",
        "  class_path = os.path.join(DATASET,class_name)\n",
        "  local_class_path = os.path.join(REPRESENTIVE_DATA_PATH,class_name)\n",
        "  try:\n",
        "    os.mkdir(local_class_path)\n",
        "  except FileExistsError:\n",
        "    print('folder already exists:' + local_class_path)\n",
        "\n",
        "  images_list = os.listdir(class_path)\n",
        "  selected_images = random.sample(images_list,DATASET_SIZE)\n",
        "  print(selected_images)\n",
        "\n",
        "  for image in selected_images:\n",
        "    dest_image_path = os.path.join(local_class_path,image)\n",
        "    orig_image = os.path.join(class_path,image) # Make the list include the whole path to the image\n",
        "    print('Copied: ' + image)\n",
        "    shutil.copy(orig_image, dest_image_path)\n"
      ],
      "metadata": {
        "id": "mwd3iHN1We0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVN running"
      ],
      "metadata": {
        "id": "6LE3DD3hoW3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first code cell is loading the necessary libraries and package dependencies required to run the code.\n",
        "The below commands install the python packages glob3, numpy, and opencv-python. glob3 is a library that allows for matching file paths with a pattern, numpy is a library for scientific computing with python, and opencv-python is a library for image processing."
      ],
      "metadata": {
        "id": "zR6cSx36pbMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install glob3\n",
        "# !pip3 install numpy\n",
        "# !pip3 install opencv-python "
      ],
      "metadata": {
        "id": "OQNk7kqq2Qs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Data\n",
        "\n",
        "In this section, we are loading the data that we will use to train and test our model. We set the base directory where the images are stored, and then initialize empty lists X and y to store the image data and labels respectively. We then use `os.listdir()` to get a list of subdirectories in the base directory, which will represent the class names. We loop over the subdirectories, using `glob.glob()` to get a list of image files in the subdirectory with the extension .jpg. We then load the image data using OpenCV, append the image data to the X list, and append the class label (i.e., the subdirectory name) to the y list. Finally, we convert the X and y lists to numpy arrays so that we can use them to train and test our model."
      ],
      "metadata": {
        "id": "g10KRXAN1JPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Set the base directory where the images are stored\n",
        "base_dir = '/content/drive/MyDrive/BM_cytomorphology_data_rep'\n",
        "\n",
        "# Initialize lists to store the image data and labels\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Get the list of subdirectories (i.e., the class names)\n",
        "subdirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "# Loop over the subdirectories\n",
        "for subdir in subdirs:\n",
        "    # Get the list of image files in the subdirectory\n",
        "    image_files = glob.glob(os.path.join(base_dir, subdir, '*.jpg'))\n",
        "\n",
        "    # Loop over the image files\n",
        "    for image in image_files:\n",
        "        print('loading: ' + image)\n",
        "        # Load the image data using OpenCV or any other image processing library\n",
        "        image = cv2.imread(image)\n",
        "        # Append the image data to the X list\n",
        "        X.append(image)\n",
        "        # Append the class label (i.e., the subdirectory name) to the y list\n",
        "        y.append(subdir)\n",
        "\n",
        "# Convert the X and y lists to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "UHDsbUs6pdH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train the model\n",
        "In this section, we are training our model.\n",
        "\n",
        "We start by updating the scikit-learn package to the latest version. Next, we import the necessary libraries and modules, including numpy, sklearn, SVC (Support Vector Classification) from sklearn.svm, OneVsRestClassifier from sklearn.multiclass, accuracy_score and train_test_split from sklearn.metrics and sklearn.model_selection. We also print out the version of scikit-learn that we are using."
      ],
      "metadata": {
        "id": "IdZ_3Jp5pwcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "Oa5ns-1O3DUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"Sklearn version \" + sklearn.__version__)"
      ],
      "metadata": {
        "id": "9ba0GIsu89J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by splitting our data into training and test sets, with a test set size of 20% and a random state of 42. Then, we create an SVM classifier with a radial basis function (rbf) kernel, and set the class weight to 'balanced' and verbose to True. We also use the OneVsRestClassifier class which allows for a multi-class problem to be reduced to multiple binary classification problems, each classifying data as either in the class or not in the class. Then we flatten the data so that it can be inputted into the classifier. Finally, we fit the classifier to the training data."
      ],
      "metadata": {
        "id": "_WeP9Ab51rwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "clf = SVC(kernel='rbf', class_weight='balanced', verbose=True)\n",
        "\n",
        "clf = OneVsRestClassifier(clf, n_jobs=-1, verbose=10)\n",
        "\n",
        "# Flatten the data\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n"
      ],
      "metadata": {
        "id": "mnkGLU-fp2FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0LhPmOnpsDZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we use our trained model to make predictions on the test data. We then print the accuracy of the classifier using `accuracy_score()`. We also import classification_report from sklearn.metrics and print out the report, which includes precision, recall, f1-score and support for each class. Then, we use `seaborn` and `matplotlib` to create a confusion matrix visualization of the classifier's performance."
      ],
      "metadata": {
        "id": "PK5lsKhe16X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the accuracy of the classifier\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-MYOglh-sB56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred,\n",
        "                            target_names=CLASSES))"
      ],
      "metadata": {
        "id": "3d_eDsEr7ho6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yJ5zgS1k70g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "mat = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(mat.T, annot=False, cmap=\"crest\",\n",
        "            xticklabels=CLASSES,\n",
        "            yticklabels=CLASSES)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "metadata": {
        "id": "QJ548dTK7rvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!killall sar"
      ],
      "metadata": {
        "id": "sz8VPJPMGohs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Store Model\n",
        "In this section, we are storing the trained model to a file.\n",
        "We start by importing the pickle library, which allows us to serialize and save the model to a file. We then use the `open()` function to create a new file in the specified directory with the name 'svm_rbf.pkl' and the mode 'wb' (write binary). We then use the `pickle.dump()` function to write the clf object (our trained model) to the file. This way we can use it later on.\n",
        "\n",
        "Once the model is stored, it can be loaded and used to make predictions on new data. The stored model can also be used to retrain the model on new data."
      ],
      "metadata": {
        "id": "luC1Njxh-O2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the model to a file\n",
        "with open('/content/drive/MyDrive/svm_rbf.pkl', 'wb') as f:\n",
        "    pickle.dump(clf, f)"
      ],
      "metadata": {
        "id": "4j4toaSN-OGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Model\n",
        "Load the data and make some predictions on it."
      ],
      "metadata": {
        "id": "G3iE4hOTDbo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/svm_rbf.pkl', 'rb') as f:\n",
        "    clf = pickle.load(f)"
      ],
      "metadata": {
        "id": "WGup3cjxDeE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install gnuplot gawk"
      ],
      "metadata": {
        "id": "WNrvG7SIVdIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!!sadf -d /content/drive/MyDrive/sar-mem-rbf.log -- -r| gawk -F\";\" '{print $3 \" \" $6}' | gawk '{gsub(/ UTC/,\"\",$2); print}' | gnuplot -persist -e \"set terminal png; set output 'memstats.png';set xdata time; set timefmt '%Y-%m-%d %H:%M:%S'; set xlabel 'Time'; set ylabel 'Memory Usage'; plot '-' using 1:4 with lines;\""
      ],
      "metadata": {
        "id": "VjxNSBpIVEwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}